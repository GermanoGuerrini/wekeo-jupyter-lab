{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/LogoWekeo_Copernicus_RGB_0.png' alt='' align='centre' width='30%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Copernicus data to investigate Marine Heatwaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Version: 3.0\n",
    "    Date:    14/07/2020\n",
    "    Author:  Hayley Evers-King (EUMETSAT) and Ben Loveday (InnoFlair, Plymouth Marine Laboratory)\n",
    "    Credit:  This code was developed for EUMETSAT under contracts for the European Commission Copernicus \n",
    "             programme.\n",
    "    License: This code is offered as open source and free-to-use in the public domain, \n",
    "             with no warranty, under the MIT license associated with this code repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this notebook for?**\n",
    "\n",
    "This notebook will download EUMETSAT Sentinel-3 SLSTR data for composite plotting, as well as CMEMS time series data of SST, from both reprocessed and NRT data stream. The notebook covers the application of some simple plotting techniques and application of basic analysis to investigate both historical and current potential for the occurrence of marine heat waves.\n",
    "\n",
    "**What specific tools does this notebook use?**\n",
    "\n",
    "Beyond general Python modules, this notebook imports some functions for managing the harmonised data access api (harmonised_data_access_api_tools.py) which can be found in the wekeo-hda folder on JupyterLab, and additional libraries for marine heatwave analysis provided openly and based on the work of <a href=\"https://github.com/ecjoliver/marineHeatWaves\"> Hobday et al.</a>.\n",
    "\n",
    "**What are marine heatwaves and how can Copernicus data be used to observe them?**\n",
    "\n",
    "Like heatwaves on land, marine heatwaves are extended periods of higher than normal temperatures. They have occurred in different areas around the world and can be caused by different oceanographic driving forces. You can find out all about them <a href=\"http://www.marineheatwaves.org/all-about-mhws.html\">here</a>. Marine heatwaves can be devastating for marine life, particularly those that can suffer from thermal stress, such as coral reefs. \n",
    "\n",
    "\n",
    "The variable drivers, and historical context for defining heatwaves regionally, mean that we need the ability to measure the range of ocean temperatures that occur all over the world at any given time, and also a long-term base-line understanding of what ‘normal’ temperatures look like.\n",
    "\n",
    "Sea surface temperature measurements from satellites can support the identification of marine heatwaves, both through the daily measurements they make, and their contributions to long-term data records. The Sentinel-3 satellites are the Copernicus programme's contribution to climate-scale monitoring of sea surface temperatures, with the Sea and Land Surface Temperature Radiometer (SLSTR) on Sentinel-3A now able to function as a reference sensor, when combining multiple sea surface temperature data sets, such as those available from the <a href=\"http://marine.copernicus.eu\">Copernicus Marine Service</a>.\n",
    "\n",
    "In this notebook we will work through an example of how you can access near-real-time data to view current SST in a region that is often affected by marine heatwaves. We will then look at this area in a long term context using a reprocessed time series, to see how the current situation compares to historical marine heat wave episodes.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Get WEkEO User credentials</b>\n",
    "<hr>\n",
    "If you want to download the data to use this notebook, you will need WEkEO User credentials. If you do not have these, you can register <a href=\"https://www.wekeo.eu/web/guest/user-registration\" target=\"_blank\">here</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started! Python is divided into a series of libraries, packages, and modules that each contain a series of methods for specific tasks. The box below imports everything we need to complete the tasks in this notebook including data access, manipulation, analysis and plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard tools\n",
    "import os, sys, json\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# specific tools (which can be found here ../../tools/)\n",
    "sys.path.append(os.path.join(os.getcwd(),'tools'))\n",
    "sys.path.append(os.path.join(os.getcwd(),'tools','mhw_master'))\n",
    "\n",
    "import image_tools as img\n",
    "import SST_plotting_tools as sstp\n",
    "import marineHeatWaves as mhw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the WEkEO HDA client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WEkEO HDA client is a python based library. It provides support for both Python 2.7.x and Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to install the WEkEO HDA client via the package management system pip, you have to running on Unix/Linux the command shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please verify the following requirements are installed before skipping to the next step:\n",
    "   - Python 3\n",
    "   - requests\n",
    "   - tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load WEkEO HDA client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hda client provides a fully compliant Python 3 client that can be used to search and download products using the Harmonized Data Access WEkEO API.\n",
    "HDA is RESTful interface allowing users to search and download WEkEO datasets.\n",
    "Documentation about its usage can be found at https://www.wekeo.eu/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hda import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this script, we will download some data from WEkEO harmonised data access. WEkEO provides access to a huge number of datasets through its **'harmonised-data-access'** API. This allows us to query the full data catalogue and download data quickly and directly onto the Jupyter Lab. You can search for what data is available <a href=\"https://wekeo.eu/data?view=catalogue\">here</a>\n",
    "\n",
    "In order to use the HDA client we need to provide some authentication credentials. Each user first makes sure the file \"$HOME/.hdarc\" exists with the URL to the API end point and your user and password.\n",
    "\n",
    "For example, to search for the file .hdarc in the $HOME diretory, the user would open a terminale and run the following command:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "find $HOME -type f -name .hdarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then he could copy the code below in the file \"$HOME/.hdarc\" (in your Unix/Linux environment) and adapt the following template with the credentials of your WEkEO account:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your URL and WEkEO API username and password (needs to be in '  ')\n",
    "url: 'https://wekeo-broker.apps.mercator.dpi.wekeo.eu/databroker'\n",
    "user_name = '#############'\n",
    "password = '#############'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will also define a few other parameters including where to download the data to, and if we want the HDA client functions to be verbose. **Lastly, we will tell the notebook where to find the query we will use to find the data.** These 'JSON' queries are what we use to ask WEkEO for data. They have a very specific form, but allow us quite fine grained control over what data to get. You can find the ones that we will use here: **JSON_templates/mhw/..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the data should be downloaded to:\n",
    "download_dir_path = os.path.join(os.getcwd(),'products')\n",
    "# where we can find our data query form:\n",
    "JSON_query_dir = os.path.join(os.getcwd(),'JSON_templates','mhw')\n",
    "# HDA-API loud and noisy?\n",
    "verbose = False\n",
    "\n",
    "# make the output directory if required\n",
    "if not os.path.exists(download_dir_path):\n",
    "    os.makedirs(download_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to get our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLSTR L2 SST KEY\n",
    "dataset_id = \"EO:EUM:DAT:SENTINEL-3:SL_2_WST___\"\n",
    "download_data = True #Set this to False if you've already downloaded the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our dataset ID to load the correct JSON query file from ../JSON_templates/mhw/\n",
    "\n",
    "You can edit this query if you want to get different data, but be aware of asking for too much data - you could be here a while and might run out of space to use this data in the JupyterLab. The box below gets the correct query file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find query file\n",
    "JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "if not os.path.exists(JSON_query_file):\n",
    "    print('Query file ' + JSON_query_file + ' does not exist')\n",
    "else:\n",
    "    print('Found JSON query file for '+dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a query, we need to launch it to WEkEO to get our data. The box below uses directly the client to download data.\n",
    "\n",
    "This is quite a complex process, so much of the functionality has been buried 'behind the scenes'. If you want more information, you can check out the <a href=\"./wekeo-hda\">Harmonised Data Access API </a></span> tutorials. The code below will report some information as it runs. At the end, it should tell you that one product has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "\n",
    "    # set maximum results to find to make sure we capture JSON pagination\n",
    "    total_results = 1e6\n",
    "\n",
    "    # load the query\n",
    "    with open(JSON_query_file, 'r') as f:\n",
    "        query = json.load(f)\n",
    "    \n",
    "    \n",
    "    # download data\n",
    "    print('Downloading data...')\n",
    "    c = Client(debug=True)\n",
    "\n",
    "    matches = c.search(query)\n",
    "    print(matches)\n",
    "    matches.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel data is usually distributed as a zip file, which contains the SAFE format data within. To use this, we must unzip the file. The code below handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    # unzip file\n",
    "    HAPI_dict = []\n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        if os.path.splitext(filename)[-1] == '.zip':\n",
    "            HAPI_dict.append(filename)\n",
    "            print('Unzipping file')\n",
    "            try:\n",
    "                with ZipFile(filename, 'r') as zipObj:\n",
    "                    # Extract all the contents of zip file in current directory\n",
    "                    zipObj.extractall(os.path.dirname(filename))\n",
    "\n",
    "                # clear up the zip file\n",
    "                os.remove(filename)\n",
    "            except:\n",
    "                print(\"Failed to unzip....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting SLSTR data\n",
    "\n",
    "We plot our SLSTR scenes using a function that manages data ingestion, flagging, bias correction and makes some map embelishements (e.g. adds dotted lines to the scene edges, so we can tell where the boundaries are). We call this function for each image in the boxes further down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by finding all the necessary files (which glob.glob takes care of). The files are added to a list which is then sent to our large function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbosity\n",
    "verbose = False\n",
    "\n",
    "# figure options\n",
    "figure_font_size = 20\n",
    "plot_extents = [-148, -120, 32, 47]\n",
    "vmin = 8\n",
    "vmax = 20\n",
    "grid_factor = 3 #sub-sample to reduce plot resolution\n",
    "# get the files\n",
    "SLSTR_files = glob.glob(os.path.join(download_dir_path,'S3*WST*202106*','*.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we pass our list of files to the plotting routine. The plotting routine returns the handles of our axes, so that we can still make some changes once the main plot is done (e.g. add the annotations). Finally, it will save the figure in the same directory as this notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the plot: we will call this as a function as it contains a 'for' loop to make the plot\n",
    "fig, axis, colbar = sstp.make_SLSTR_composite_plot(SLSTR_files, plot_extents=plot_extents,\\\n",
    "                                              fsz=figure_font_size, vmin=vmin, vmax=vmax, grid_factor=grid_factor)\n",
    "\n",
    "# add some embellishments\n",
    "plt.savefig('SLSTR_All_SST_California_20210617.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now find the image in the folder where your code is stored in your instance of the JupyterLab. The image is a composite of three images from the SLSTR sensors aboard the Sentinel-3A and B satellites which captured warm temperatures around the eastern Pacific in June 2021. This highlights the increased coverage that can be achieved in both time and space, using multiple sensors, but in order to understand if these temperatures could be an indicator of the beginning of a marine heatwave we need some longer term context..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at time series data of sea surface temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To place the images above in context we'll need to look at a time series of data. For this we can access data from the Copernicus services, where multiple data sources (different satellites etc) are combined to produce data sets that cover longer time periods. In this case we are going to look at the OSTIA Sea Surface Temperature using both the NRT and reprocessed data streams, so we can look at data right up to the time period of the SLSTR images we looked at previously. \n",
    "\n",
    "Reprocessed and NRT data streams are produced separately and we must be careful when we interpret these data sets together. As time progresses, we understand better how satellite instruments perform, algorithms improve, and data is reprocessed to ensure good quality and consistency between sources. With NRT we do not have all the information we have in hindsight, particularly about instrument characterisation, but this data is vitally important for understanding events that are happening right now. So, for example, you would not use combined NRT and reprocessed data sets for long-term trend analysis, and you would want to consider NRT measurements with a higher degree of uncertainty that you might consider with reprocessed data. So whilst we can use the NRT data now to get an indication of whether this is event is looking like it might be significant, we would eventually want to use a longer term time series that have been reprocessed, to establish how unusual this is event is in a more climatic context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will construct and submit a query to the WEkEO Harmonised Data Access API to get this data. Here we have supplied the JSON files for the data sets of interest, but you could edit these files to look at your own time frames/regions of interest etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a date array for the REP product\n",
    "start_dates = []\n",
    "end_dates = []\n",
    "dataset_ids = []\n",
    "for ii in range(2008,2019+1):\n",
    "    start_dates.append(str(ii)+\"-01-01T00:00:00.000Z\")\n",
    "    end_dates.append(str(ii)+\"-12-31T00:00:00.000Z\")\n",
    "    dataset_ids.append(\"EO:MO:DAT:SST_GLO_SST_L4_REP_OBSERVATIONS_010_011\")\n",
    "\n",
    "# add the NRT product\n",
    "for ii in range(2020,2021+1):\n",
    "    start_dates.append(str(ii)+\"-01-01T00:00:00.000Z\")\n",
    "    end_dates.append(str(ii)+\"-12-31T00:00:00.000Z\")\n",
    "    dataset_ids.append(\"EO:MO:DAT:SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001\")\n",
    "\n",
    "# start loop over dates\n",
    "if download_data:\n",
    "\n",
    "    for start_date, end_date, dataset_id in zip(start_dates, end_dates, dataset_ids):\n",
    "        \n",
    "        # find query file\n",
    "        JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "        if not os.path.exists(JSON_query_file):\n",
    "            print('Query file ' + JSON_query_file + ' does not exist')\n",
    "        else:\n",
    "            print('Found JSON query file for '+dataset_id)\n",
    "\n",
    "        print('Running for: ' + start_date + ' to ' + end_date)\n",
    "\n",
    "        date_tag = start_date.split('T')[0] + '_' + end_date.split('T')[0]\n",
    "        date_string = start_date.split('T')[0].replace('-','') \\\n",
    "                      + '_' + end_date.split('T')[0].replace('-','')\n",
    "\n",
    "        # load the query\n",
    "        with open(JSON_query_file, 'r') as f:\n",
    "            query = f.read()\n",
    "            query = query.replace(\"%DATE_START%\",start_date)\n",
    "            query = query.replace(\"%DATE_END%\",end_date)\n",
    "            query = json.loads(query)\n",
    "        \n",
    "\n",
    "        # download data\n",
    "        print('Downloading data...')\n",
    "        c = Client(debug=True)\n",
    "\n",
    "        matches = c.search(query)\n",
    "        print(matches)\n",
    "        matches.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting time series data to investigate occurrence and potential for marine heatwaves "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will set up some parameters including times and space we want to work over when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data product time is measured in seconds since the following refernce date\n",
    "Date_ref = datetime.datetime(1981,1,1)\n",
    "\n",
    "# Select the times we want to use for our spatial anomaly plots [month, day]\n",
    "month_day_start = [1,1]\n",
    "month_day_end = [12,31]\n",
    "\n",
    "# Plotting font size\n",
    "fsz = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will find our datasets and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_files = []\n",
    "my_files = glob.glob(os.path.join(download_dir_path,'METOFFICE-GLO-SST-L4-*'))\n",
    "for SST_file in sorted(my_files):\n",
    "    SST_files.append(SST_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs the bulk of the processing work for our plot. We start by loading in coordinates that we can use to subset the data (if required) and make our plots. Subsequently, we loop through the SST products and read in the data, correct Kelvin to Celsius and perform our averaging in either time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the co-ordinate variables we need for subsetting/plotting\n",
    "ds = xr.open_dataset(SST_files[-1])\n",
    "lat = ds.lat.data\n",
    "lon = ds.lon.data\n",
    "ds.close()\n",
    "\n",
    "# initialise lists for output times series variables for MWH    \n",
    "all_times = []\n",
    "all_SST = []\n",
    "\n",
    "# initialise arrays for output SST fields\n",
    "iter_SST = np.ones([len(SST_files), len(lat), len(lon)])*np.nan\n",
    "\n",
    "# now we get the area-averaged data\n",
    "count = -1\n",
    "for SST_file in SST_files:\n",
    "    count = count + 1\n",
    "\n",
    "    # xarray does not read times consistently between RAN and NRT, so we load as integer\n",
    "    ds = xr.open_dataset(SST_file, decode_times=False)\n",
    "    this_SST = ds.analysed_sst.data\n",
    "    times = ds.time.data\n",
    "    ds.close()\n",
    "\n",
    "    my_times = []\n",
    "    for time in times:\n",
    "        my_times.append(Date_ref + datetime.timedelta(seconds=int(time)))\n",
    "    times = np.asarray(my_times)\n",
    "    \n",
    "    t0 = datetime.datetime(times[0].year, month_day_start[0], month_day_start[1])\n",
    "    t1 = datetime.datetime(times[0].year, month_day_end[0], month_day_end[1])\n",
    "    tt = np.where((times >= t0) & (times <= t1))[0]\n",
    "\n",
    "    if np.nanmean(this_SST) > 100:\n",
    "        this_SST = this_SST - 273.15\n",
    "\n",
    "    time_subset_SST = np.nanmean(np.nanmean(this_SST[tt,:,:],axis=1),axis=1)\n",
    "\n",
    "    iter_SST[count,:,:] = np.squeeze(np.nanmean(time_subset_SST, axis=0))\n",
    "\n",
    "    all_times.append(my_times)\n",
    "    all_SST.append(np.nanmean(np.nanmean(this_SST, axis=1), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of formatting is necessary for the plots we want to make..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the SST list\n",
    "SST_time_series = [item for sublist in all_SST for item in sublist]\n",
    "SST_time_series = np.asarray(SST_time_series)\n",
    "\n",
    "# format the dates for the MWH toolkit\n",
    "Dates_time_series = [item for sublist in all_times for item in sublist]\n",
    "Dates_time_series_formatted = [datetime.date.toordinal(tt) for tt in Dates_time_series]\n",
    "\n",
    "# make climatology of time subset region\n",
    "clim_SST = np.nanmean(iter_SST, axis=0)\n",
    "\n",
    "# calculate heat waves\n",
    "mhws, clim = mhw.detect(np.asarray(Dates_time_series_formatted), SST_time_series)\n",
    "\n",
    "# make matrix\n",
    "stripe_array = np.ones([20,len(SST_files)])*np.nan\n",
    "\n",
    "# now make the anomaly\n",
    "for ii in range(len(SST_files)):\n",
    "    stripe_array[:, ii] = np.nanmean(np.squeeze(iter_SST[ii,:,:]) - clim_SST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot we'll make, shows ‘stripes’ of the average sea surface temperature anomaly for our region of interest. Recently, ‘climate stripes’  have been used by 'citizen scientists' all over the world to show long-term trends in regional temperatures. The plot below shows a stripes-style graphic derived using the SST time series we have extracted. High anomalies are apparent during 2014, in 2015 during the previous marine heatwave, and were associated with reports in 2019 and 2020.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,10), dpi =300)\n",
    "vmax = np.nanmax(abs(stripe_array))\n",
    "\n",
    "date_ticks = []\n",
    "for ii in range(len(SST_files)):\n",
    "    date_ticks.append(str(2008+ii))\n",
    "    \n",
    "plt.pcolormesh(stripe_array[:,:],vmin=vmax*-1,vmax=vmax,cmap=plt.cm.RdBu_r)\n",
    "plt.xticks(np.arange(len(SST_files))+0.5,date_ticks, rotation='90', fontsize=fsz/1.25)\n",
    "plt.xlim([0,len(SST_files)-1])\n",
    "plt.yticks([],[], fontsize=fsz/1.25)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('SST anomaly [$^{o}$C]',fontsize=fsz/1.25)\n",
    "plt.savefig('Stripes.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, we will look a little more quantitatively at historical marine heatwaves in this region, and see how the situation in recent times compares. The plot below is generated using a \n",
    "a toolkit very kindly provided as open source code by Hobday et al (you can find out more information on the toolkit and the science behind it <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0079661116000057\">here</a> and <a href=\"https://github.com/ecjoliver/marineHeatWaves\">here</a>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MWHs\n",
    "ev = np.argmax(mhws['intensity_max']) # Find largest event\n",
    "\n",
    "fig = plt.figure(figsize=(35,15), dpi = 300)\n",
    "plt.rc('font', size=fsz)\n",
    "\n",
    "# Find indices for all n-MHWs before and after event of interest and shade accordingly\n",
    "n_before=10\n",
    "n_after=10\n",
    "for ev0 in np.arange(ev-n_before, ev+n_after, 1):\n",
    "    t1 = np.where(Dates_time_series_formatted==mhws['time_start'][ev0])[0][0]\n",
    "    t2 = np.where(Dates_time_series_formatted==mhws['time_end'][ev0])[0][0]\n",
    "    p1 = plt.fill_between(Dates_time_series[t1:t2+1], SST_time_series[t1:t2+1],\\\n",
    "                          clim['thresh'][t1:t2+1], color=(1,0.85,0.85))\n",
    "    \n",
    "# Find indices for MHW of interest and shade accordingly\n",
    "t1 = np.where(Dates_time_series_formatted==mhws['time_start'][-1])[0][0]\n",
    "t2 = np.where(Dates_time_series_formatted==mhws['time_end'][-1])[0][0]\n",
    "p2 = plt.fill_between(Dates_time_series[t1:t2+1], SST_time_series[t1:t2+1],\\\n",
    "                      clim['thresh'][t1:t2+1], color='r')\n",
    "\n",
    "# Plot SST, seasonal cycle, threshold, shade MHWs with main event in red\n",
    "p3, = plt.plot(Dates_time_series, SST_time_series, 'k-', linewidth=2)\n",
    "p4, = plt.plot(Dates_time_series, clim['thresh'], 'b--', linewidth=2)\n",
    "p5, = plt.plot(Dates_time_series, clim['seas'], 'b-', linewidth=2)\n",
    "\n",
    "xmin = datetime.datetime(2013,1,1)\n",
    "xmax = datetime.datetime(2021,12,31)\n",
    "plt.xlim(xmin,xmax)\n",
    "\n",
    "plt.ylim(clim['seas'].min() - 0.3, clim['seas'].max() + mhws['intensity_max'][ev] + 0.2)\n",
    "plt.ylabel('SST [$^\\circ$C]')\n",
    "plt.annotate('Plotting script credit:\\nhttps://github.com/ecjoliver/marineHeatWaves',\\\n",
    "             (0.005, 0.935), xycoords='axes fraction', color='0.5', fontsize=fsz/1.25)\n",
    "\n",
    "leg = plt.legend([p3, p5, p4, p2, p1],\\\n",
    "                 ['OSTIA SST','SST Seas. clim.','SST Seas. thresh.','Recent heatwave','Past heatwave'],\\\n",
    "                 bbox_to_anchor=(0.5, -0.3), ncol=5, loc=\"lower center\")\n",
    "\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "plt.savefig('MHW.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an example of the sorts of analyses that can be developed with SST data for marine heatwaves. The diversity of data available through Copernicus programme allows for the investigation of this phenomena at both the event and climate scales. To extend this analysis you could use a longer time series of solely reprocessed data to determine climate related trends, you could also routinely compare NRT data to a climatology (with the caveat of greater uncertainty associated with the NRT data source) for any region of interest. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/all_partners_wekeo.png' alt='' align='center' width='75%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left;\">This project is licensed under the <a href=\"./LICENSE\">MIT License</a> <span style=\"float:right;\"><a href=\"https://github.com/wekeo/wekeo-jupyter-lab\">View on GitHub</a> | <a href=\"https://www.wekeo.eu/\">WEkEO Website</a> | <a href=mailto:support@wekeo.eu>Contact</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
